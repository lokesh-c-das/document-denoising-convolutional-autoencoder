{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Kaggle installation "
      ],
      "metadata": {
        "id": "9s-f0Wf7YOVG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3SGeyOey1kYB"
      },
      "outputs": [],
      "source": [
        "! pip install kaggle\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wkMUgeOEcQLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qWcCC2XU15Df"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir dataset\n",
        "! cd dataset\n",
        "! kaggle competitions download -c denoising-dirty-documents\n",
        "! unzip denoising-dirty-documents.zip\n",
        "! unzip test.zip\n",
        "! unzip train.zip\n",
        "! unzip train_cleaned.zip\n"
      ],
      "metadata": {
        "id": "x4IS8sfpVwRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ROOT =\"/content/\""
      ],
      "metadata": {
        "id": "nPlzdzdQWzyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-python"
      ],
      "metadata": {
        "id": "1DXUgSqzXPXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2"
      ],
      "metadata": {
        "id": "41nKMK8-aHsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.layers import Dense, Input, Conv2D, MaxPool2D, Activation, Flatten, Dropout, BatchNormalization, UpSampling2D\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, TensorBoard"
      ],
      "metadata": {
        "id": "QqTujkyVaKk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read images"
      ],
      "metadata": {
        "id": "zZDd4FxNspX6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PATH =\"\"+ROOT\n",
        "features = []\n",
        "test_features = []\n",
        "# read train images\n",
        "train_path = PATH+\"train/\"\n",
        "for file in os.listdir(train_path):\n",
        "    features.append(train_path+file)\n",
        "\n",
        "test_path = PATH+\"test/\"\n",
        "for file in os.listdir(test_path):\n",
        "    test_features.append(test_path+file)\n",
        "targets = []\n",
        "target_path = PATH+\"train_cleaned/\"\n",
        "for file in os.listdir(target_path):\n",
        "    targets.append(target_path+file)"
      ],
      "metadata": {
        "id": "_OSskUGBaRA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check a few images from train, test and label sets"
      ],
      "metadata": {
        "id": "a_Dq1Azhstd9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def displayImage(image_name=None, img_title=\"\"):\n",
        "    if image_name:\n",
        "        from matplotlib import image\n",
        "        img = image.imread(image_name) # train image \n",
        "        plt.imshow(img)\n",
        "        plt.title(img_title)\n",
        "        plt.gray()\n",
        "        plt.show()\n",
        "    \n",
        "    return None\n"
      ],
      "metadata": {
        "id": "Tpz1LA-laXNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# check with random images from train, test and target sets"
      ],
      "metadata": {
        "id": "Rm0Pun7XsxxK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "displayImage(features[0],\"Image with Noise\") # Train Image"
      ],
      "metadata": {
        "id": "QwfabwFBaZXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "displayImage(targets[0],\"Image after denoising\") # Target Image"
      ],
      "metadata": {
        "id": "yIC685nxadiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# For feeding into the ML model, all images should have same shape. We will check if all iamges have same dimension.\n"
      ],
      "metadata": {
        "id": "nBbl6NQts7N2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def checkImageDimension(images):\n",
        "    height = []\n",
        "    width = []\n",
        "    for image in images:\n",
        "        img = cv2.imread(image)\n",
        "        height.append(img.shape[0])\n",
        "        width.append(img.shape[1])\n",
        "    \n",
        "    print(f'(Min Height, Max Height):{min(height), max(height)}')\n",
        "    print(f'(Min Width, Max Width):{min(width), max(width)}')"
      ],
      "metadata": {
        "id": "bHv-_en3aggY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkImageDimension(features)"
      ],
      "metadata": {
        "id": "ugXmdStxajEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# It's clearlly visible that some images have different heights. So, it would be wise to make the images of same height. So, our target images dimension would be: 258x540x3"
      ],
      "metadata": {
        "id": "Thc5t639tAya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# resize and normalize images between 0 and 1\n",
        "\n",
        "def resizeNormalizeImages(images):\n",
        "    new_shape = (540,258)\n",
        "    preprocessed_images = []\n",
        "    for image in images:\n",
        "        img = cv2.imread(image,cv2.IMREAD_GRAYSCALE)\n",
        "        img = cv2.resize(img, new_shape)\n",
        "        img = img/255.\n",
        "        img = np.expand_dims(img, axis=-1)\n",
        "        preprocessed_images.append(img)\n",
        "    return preprocessed_images\n",
        "\n"
      ],
      "metadata": {
        "id": "fIWfOXKOalPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate train, test and target sets\n",
        "processed_features = np.array(resizeNormalizeImages(features))\n",
        "processed_targets = np.array(resizeNormalizeImages(targets))\n",
        "processed_test = np.array(resizeNormalizeImages(test_features))"
      ],
      "metadata": {
        "id": "3SGZLxjZanv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_features.shape"
      ],
      "metadata": {
        "id": "wEIHH_sAaqK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implement Machine Learning model:\n",
        "## Split data(train/target sets) set for training and validation\n"
      ],
      "metadata": {
        "id": "xhsIRwgJtGeo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(processed_features, processed_targets,test_size=0.3, random_state=540)"
      ],
      "metadata": {
        "id": "QJQQI7rDaspy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check training and validation dataset sizes\n",
        "print(f'Training set Size: {X_train.shape}')\n",
        "print(f'Validation set Size: {X_val.shape}')"
      ],
      "metadata": {
        "id": "QCKPYkEvauqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate Models: Encoder Decoder"
      ],
      "metadata": {
        "id": "VTUxs2a3tL0G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input Layer \n",
        "input_layer = Input(shape=(258,540,1))\n",
        "model = Sequential()\n",
        "\n",
        "# Encoder Part:\n",
        "model.add(Conv2D(input_shape=(258,540,1), filters=256,kernel_size=(3,3), activation='relu', padding='same', name=\"en_conv_1\"))\n",
        "model.add(BatchNormalization(name=\"en_bn_1\"))\n",
        "model.add(MaxPool2D((2,2), padding='same',name='en_max_pool_1'))\n",
        "#decoder\n",
        "model.add(Conv2D(filters=256,kernel_size=(3,3),activation='relu',padding='same', name='de_conv_1'))\n",
        "model.add(UpSampling2D((2,2), name='upsampling_1'))\n",
        "model.add(Conv2D(filters=1, kernel_size=(3,3), activation='sigmoid', padding='same', name='de_conv_2'))\n",
        "\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "LN-4QxFKaxOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['RootMeanSquaredError'])\n",
        "history = model.fit(X_train,y_train, batch_size=16, epochs=200, validation_data=(X_val,y_val))"
      ],
      "metadata": {
        "id": "JxIw54nTa0jB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot model loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Training/Validation Loss')\n",
        "plt.legend(['Training Loss','Validation Loss'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "r9e0Sb32om8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(processed_test)\n",
        "predictions.shape"
      ],
      "metadata": {
        "id": "Fus1ICjCcR46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions.shape[0]"
      ],
      "metadata": {
        "id": "xMWEa7LVcoeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0,processed_test.shape[0]):\n",
        "  f, axs = plt.subplots(1,2,figsize=(10,5))\n",
        "  axs[0].imshow(processed_test[i].reshape(258,540),cmap='gray')\n",
        "  axs[0].title.set_text('With Noise Image '+str(i+1))\n",
        "  axs[1].imshow(predictions[i].reshape(258,540),cmap='gray')\n",
        "  axs[1].title.set_text(\"After denoising Image \"+str(i+1))\n"
      ],
      "metadata": {
        "id": "7on_1nKEkY9l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}